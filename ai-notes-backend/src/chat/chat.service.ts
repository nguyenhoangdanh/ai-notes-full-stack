import { Injectable } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import { VectorsService } from '../vectors/vectors.service';
import { SettingsService } from '../settings/settings.service';
import { OpenAI } from 'openai';

@Injectable()
export class ChatService {
  private openaiApiKey: string;
  private geminiApiKey: string;

  constructor(
    private configService: ConfigService,
    private vectorsService: VectorsService,
    private settingsService: SettingsService,
  ) {
    this.openaiApiKey = configService.get('OPENAI_API_KEY');
    this.geminiApiKey = configService.get('GOOGLE_GEMINI_API_KEY');
    
    if (this.geminiApiKey) {
      console.log('‚úÖ Google Gemini API key configured (Free tier available)');
    } else if (this.openaiApiKey) {
      console.log('‚úÖ OpenAI API key configured');
    } else {
      console.log('‚ö†Ô∏è No AI API key configured');
    }
  }

  async streamResponse(query: string, userId: string) {
    try {
      const settings = await this.settingsService.findByUserId(userId);
      let model = settings?.model || 'gemini-1.5-flash';
      const maxTokens = settings?.maxTokens || 4000;

      console.log('Chat stream request:', { query, userId, model });

      if (model.startsWith('gpt-') && !this.openaiApiKey) {
        model = 'gemini-1.5-flash';
        console.log('Switching to Gemini due to missing OpenAI key');
      }

      const { context, citations } = await this.vectorsService.buildChatContext(
        query,
        userId,
        Math.floor(maxTokens * 0.7),
      );

      const systemPrompt = this.buildSystemPrompt();
      const fullPrompt = context.trim()
        ? `${systemPrompt}\n\nContext from your notes:\n${context}\n\nUser question: ${query}\n\nPlease answer based on the context provided above.`
        : `${systemPrompt}\n\nNo relevant context was found in your notes for this query: "${query}"\n\nPlease inform the user that you couldn't find this information in their notes and suggest they might want to add relevant notes on this topic.`;

      console.log('Full prompt length:', fullPrompt.length);
      console.log('Citations found:', citations.length);

      if (this.geminiApiKey && (model.includes('gemini') || model.startsWith('gpt-'))) {
        const geminiModel = model.startsWith('gpt-') ? 'gemini-1.5-flash' : model;
        console.log(`Using Gemini model: ${geminiModel}`);
        return await this.streamWithGemini(fullPrompt, geminiModel, citations);
      }  else if (this.openaiApiKey && model.startsWith('gpt-')) {
        return await this.streamWithOpenAI(fullPrompt, model, maxTokens, citations);
      } else {
        if (this.geminiApiKey) {
          console.log('Falling back to Gemini');
          return await this.streamWithGemini(fullPrompt, 'gemini-1.5-flash', citations);
        }  else {
          return this.createFallbackStream('No AI API key configured. Please add GOOGLE_GEMINI_API_KEY (free), GROQ_API_KEY (free), or OPENAI_API_KEY to environment variables.', citations);
        }
      }
    } catch (error) {
      console.error('Error in streamResponse:', error);
      
      if (error.code === 'insufficient_quota' || error.status === 429) {
        console.log('OpenAI quota exceeded, trying free alternatives...');
        
        if (this.geminiApiKey) {
          try {
            console.log('Switching to Gemini due to OpenAI quota exceeded');
            const { context, citations } = await this.vectorsService.buildChatContext(query, userId, 3000);
            const systemPrompt = this.buildSystemPrompt();
            const fullPrompt = context.trim()
              ? `${systemPrompt}\n\nContext from your notes:\n${context}\n\nUser question: ${query}\n\nPlease answer based on the context provided above.`
              : `${systemPrompt}\n\nNo relevant context was found in your notes for this query: "${query}"\n\nPlease inform the user that you couldn't find this information in their notes and suggest they might want to add relevant notes on this topic.`;
            
            return await this.streamWithGemini(fullPrompt, 'gemini-1.5-flash', citations);
          } catch (geminiError) {
            console.error('Gemini also failed:', geminiError);
          }
        }
        
        const fallbackMessage = 'OpenAI quota exceeded. Google Gemini configured but also failed. Please check your API keys or try again later.';
        return this.createFallbackStream(fallbackMessage, []);
      }
      
      const fallbackMessage = 'Sorry, I encountered an error while processing your request. Please try again later.';
      return this.createFallbackStream(fallbackMessage, []);
    }
  }

  private async streamWithGemini(prompt: string, model: string, citations: any[]) {
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${this.geminiApiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [{
          parts: [{ text: prompt }]
        }],
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 1000,
          topP: 0.8,
          topK: 10
        }
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Gemini API error response:', errorText);
      throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    console.log('Gemini API response:', data);

    const fullText = data.candidates?.[0]?.content?.parts?.[0]?.text || 'No response generated';
    
    const stream = {
      async *[Symbol.asyncIterator]() {
        const words = fullText.split(' ');
        
        for (let i = 0; i < words.length; i += 3) {
          const chunk = words.slice(i, i + 3).join(' ');
          const content = i + 3 >= words.length ? chunk : chunk + ' ';
          
          yield {
            choices: [{
              delta: { content }
            }]
          };
          
          await new Promise(resolve => setTimeout(resolve, 50));
        }
      }
    };

    return { stream, citations };
  }


  private async streamWithOpenAI(prompt: string, model: string, maxTokens: number, citations: any[]) {
    const openai = new OpenAI({ apiKey: this.openaiApiKey });

    const stream = await openai.chat.completions.create({
      model: model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: Math.floor(maxTokens * 0.3),
      temperature: 0.7,
      stream: true,
    });

    return { stream, citations };
  }

  async completeResponse(query: string, userId: string) {
    try {
      const settings = await this.settingsService.findByUserId(userId);
      let model = settings?.model || 'gemini-1.5-flash';
      const maxTokens = settings?.maxTokens || 4000;

      if (model.startsWith('gpt-') && !this.openaiApiKey) {
        model = 'gemini-1.5-flash';
      }

      const { context, citations } = await this.vectorsService.buildChatContext(
        query,
        userId,
        Math.floor(maxTokens * 0.7),
      );

      const systemPrompt = this.buildSystemPrompt();
      const fullPrompt = context.trim()
        ? `${systemPrompt}\n\nContext from your notes:\n${context}\n\nUser question: ${query}\n\nPlease answer based on the context provided above.`
        : `${systemPrompt}\n\nNo relevant context was found in your notes for this query: "${query}"\n\nPlease inform the user that you couldn't find this information in their notes and suggest they might want to add relevant notes on this topic.`;

      if (this.geminiApiKey && (model.includes('gemini') || model.startsWith('gpt-'))) {
        const geminiModel = model.startsWith('gpt-') ? 'gemini-1.5-flash' : model;
        return await this.completeWithGemini(fullPrompt, geminiModel, citations);
      }  else if (this.openaiApiKey && model.startsWith('gpt-')) {
        return await this.completeWithOpenAI(fullPrompt, model, maxTokens, citations, userId);
      } else {
        if (this.geminiApiKey) {
          return await this.completeWithGemini(fullPrompt, 'gemini-1.5-flash', citations);
        } else {
          return {
            response: 'No AI API key configured. Please add GOOGLE_GEMINI_API_KEY (free) or OPENAI_API_KEY to environment variables.',
            citations: [],
          };
        }
      }
    } catch (error) {
      console.error('Error in completeResponse:', error);
      
      if (error.code === 'insufficient_quota' || error.status === 429) {
        if (this.geminiApiKey) {
          try {
            const { context, citations } = await this.vectorsService.buildChatContext(query, userId, 3000);
            const systemPrompt = this.buildSystemPrompt();
            const fullPrompt = context.trim()
              ? `${systemPrompt}\n\nContext from your notes:\n${context}\n\nUser question: ${query}\n\nPlease answer based on the context provided above.`
              : `${systemPrompt}\n\nNo relevant context was found in your notes for this query: "${query}"\n\nPlease inform the user that you couldn't find this information in their notes and suggest they might want to add relevant notes on this topic.`;
            
            return await this.completeWithGemini(fullPrompt, 'gemini-1.5-flash', citations);
          } catch (geminiError) {
            console.error('Gemini fallback failed:', geminiError);
          }
        }
        
        return {
          response: 'AI quota exceeded. Please try switching to Google Gemini (free) in Settings.',
          citations: [],
        };
      }
      
      return {
        response: 'Sorry, I encountered an error while processing your request. Please try again later.',
        citations: [],
      };
    }
  }

  private async completeWithGemini(prompt: string, model: string, citations: any[]) {
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${this.geminiApiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [{
          parts: [{ text: prompt }]
        }],
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 1000,
          topP: 0.8,
          topK: 10
        }
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Gemini API error response:', errorText);
      throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    const responseText = data.candidates?.[0]?.content?.parts?.[0]?.text || 'No response generated';

    return {
      response: responseText,
      citations,
    };
  }

  private async completeWithOpenAI(prompt: string, model: string, maxTokens: number, citations: any[], userId: string) {
    const openai = new OpenAI({ apiKey: this.openaiApiKey });

    const completion = await openai.chat.completions.create({
      model: model,
      messages: [{ role: 'user', content: prompt }],
      max_tokens: Math.floor(maxTokens * 0.3),
      temperature: 0.7,
    });

    const response = completion.choices[0]?.message?.content || '';

    const today = new Date().toISOString().split('T')[0];
    const estimatedTokens = completion.usage?.total_tokens || Math.ceil((prompt.length + response.length) / 4);
    
    this.vectorsService.updateUsage(userId, today, 0, estimatedTokens).catch(console.error);

    return {
      response,
      citations,
    };
  }

  private createFallbackStream(message: string, citations: any[]) {
    const stream = {
      async *[Symbol.asyncIterator]() {
        const words = message.split(' ');
        for (let i = 0; i < words.length; i += 3) {
          const chunk = words.slice(i, i + 3).join(' ') + ' ';
          yield {
            choices: [{
              delta: { content: chunk }
            }]
          };
          await new Promise(resolve => setTimeout(resolve, 100));
        }
      }
    };

    return { stream, citations };
  }

  private buildSystemPrompt(): string {
    return `B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh cho ·ª©ng d·ª•ng ghi ch√∫ c√° nh√¢n AI Notes. Nhi·ªám v·ª• c·ªßa b·∫°n l√† gi√∫p ng∆∞·ªùi d√πng t√¨m ki·∫øm th√¥ng tin t·ª´ ch√≠nh nh·ªØng ghi ch√∫ m√† h·ªç ƒë√£ vi·∫øt v√† tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n n·ªôi dung ƒë√≥.

üéØ NHI·ªÜM V·ª§ CH√çNH:
- Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a CH√çNH X√ÅC tr√™n n·ªôi dung ghi ch√∫ ƒë∆∞·ª£c cung c·∫•p
- T√¨m ki·∫øm v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ghi ch√∫ n·∫øu c·∫ßn
- Cung c·∫•p tr√≠ch d·∫´n r√µ r√†ng t·ª´ c√°c ghi ch√∫ g·ªëc

üìã QUY T·∫ÆC QUAN TR·ªåNG:
1. CH·ªà s·ª≠ d·ª•ng th√¥ng tin c√≥ trong ph·∫ßn "Context from your notes" ƒë∆∞·ª£c cung c·∫•p
2. N·∫øu kh√¥ng t√¨m th·∫•y th√¥ng tin trong ghi ch√∫, h√£y n√≥i: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y trong ghi ch√∫ c·ªßa b·∫°n."
3. Lu√¥n tr√≠ch d·∫´n ngu·ªìn: t√™n ghi ch√∫ v√† ti√™u ƒë·ªÅ ph·∫ßn (n·∫øu c√≥)
4. Khi c√≥ nhi·ªÅu ghi ch√∫ li√™n quan, h√£y t·ªïng h·ª£p th√¥ng tin nh∆∞ng v·∫´n gi·ªØ tr√≠ch d·∫´n
5. KH√îNG bao gi·ªù t·ª± suy di·ªÖn hay t·∫°o th√¥ng tin kh√¥ng c√≥ trong ghi ch√∫

üí° C√ÅCH TR·∫¢ L·ªúI T·ªêT:
- S·ª≠ d·ª•ng markdown ƒë·ªÉ format r√µ r√†ng
- Tr·∫£ l·ªùi ng·∫Øn g·ªçn nh∆∞ng ƒë·∫ßy ƒë·ªß
- Lu√¥n k·∫øt th√∫c v·ªõi ph·∫ßn "**Ngu·ªìn tham kh·∫£o:**" li·ªát k√™ c√°c ghi ch√∫ ƒë√£ s·ª≠ d·ª•ng
- S·ª≠ d·ª•ng ti·∫øng Vi·ªát t·ª± nhi√™n, th√¢n thi·ªán

üîç KHI KH√îNG T√åM TH·∫§Y TH√îNG TIN:
"T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ [ch·ªß ƒë·ªÅ] trong ghi ch√∫ c·ªßa b·∫°n. B·∫°n c√≥ mu·ªën th√™m ghi ch√∫ m·ªõi v·ªÅ ch·ªß ƒë·ªÅ n√†y kh√¥ng?"

H√£y nh·ªõ: B·∫°n l√† tr·ª£ l√Ω th√¥ng minh gi√∫p ng∆∞·ªùi d√πng kh√°m ph√° l·∫°i ki·∫øn th·ª©c t·ª´ ch√≠nh nh·ªØng ghi ch√∫ c·ªßa h·ªç!`;
  }

  async generateSuggestion(
    content: string,
    selectedText: string,
    suggestionType: string,
    userId: string,
    targetLanguage?: string
  ) {
    const suggestions = {
      improve: `Vai tr√≤: B·∫°n l√† editor chuy√™n nghi·ªáp.
Nhi·ªám v·ª•: C·∫£i thi·ªán vƒÉn phong c·ªßa ƒëo·∫°n text sau. L√†m cho n√≥ r√µ r√†ng, m·∫°ch l·∫°c v√† d·ªÖ hi·ªÉu h∆°n.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ phi√™n b·∫£n ƒë√£ c·∫£i thi·ªán, kh√¥ng gi·∫£i th√≠ch.`,

      expand: `Vai tr√≤: B·∫°n l√† chuy√™n gia n·ªôi dung.
Nhi·ªám v·ª•: M·ªü r·ªông ƒëo·∫°n text sau b·∫±ng c√°ch th√™m chi ti·∫øt, v√≠ d·ª• v√† gi·∫£i th√≠ch s√¢u h∆°n.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ phi√™n b·∫£n ƒë√£ m·ªü r·ªông, kh√¥ng gi·∫£i th√≠ch.`,

      summarize: `Vai tr√≤: B·∫°n l√† chuy√™n gia t√≥m t·∫Øt.
Nhi·ªám v·ª•: T√≥m t·∫Øt ƒëo·∫°n text sau th√†nh nh·ªØng √Ω ch√≠nh quan tr·ªçng nh·∫•t.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ b·∫£n t√≥m t·∫Øt, kh√¥ng gi·∫£i th√≠ch.`,

      restructure: `Vai tr√≤: B·∫°n l√† chuy√™n gia t·ªï ch·ª©c th√¥ng tin.
Nhi·ªám v·ª•: S·∫Øp x·∫øp l·∫°i c·∫•u tr√∫c ƒëo·∫°n text sau cho logic v√† d·ªÖ theo d√µi h∆°n.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ phi√™n b·∫£n ƒë√£ s·∫Øp x·∫øp l·∫°i, kh√¥ng gi·∫£i th√≠ch.`,

      examples: `Vai tr√≤: B·∫°n l√† chuy√™n gia minh h·ªça.
Nhi·ªám v·ª•: Th√™m v√≠ d·ª• c·ª• th·ªÉ v√† th·ª±c t·∫ø v√†o ƒëo·∫°n text sau.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ phi√™n b·∫£n c√≥ v√≠ d·ª•, kh√¥ng gi·∫£i th√≠ch.`,

      grammar: `Vai tr√≤: B·∫°n l√† editor ng·ªØ ph√°p.
Nhi·ªám v·ª•: S·ª≠a l·ªói ch√≠nh t·∫£, ng·ªØ ph√°p trong ƒëo·∫°n text sau.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ phi√™n b·∫£n ƒë√£ s·ª≠a l·ªói, kh√¥ng gi·∫£i th√≠ch.`,

      translate: `Vai tr√≤: B·∫°n l√† d·ªãch gi·∫£ chuy√™n nghi·ªáp.
Nhi·ªám v·ª•: D·ªãch ƒëo·∫°n text sau sang ${targetLanguage || 'ti·∫øng Anh'} m·ªôt c√°ch t·ª± nhi√™n.
Y√™u c·∫ßu: Ch·ªâ tr·∫£ v·ªÅ b·∫£n d·ªãch, kh√¥ng gi·∫£i th√≠ch.`
    };

    const targetText = selectedText || content;
    
    const systemPrompt = `${suggestions[suggestionType]}

ƒêO·∫†N TEXT C·∫¶N X·ª¨ L√ù:
${targetText}

K·∫æT QU·∫¢:`;

    try {
      const result = await this.directAICall(systemPrompt);
      
      return {
        originalText: targetText,
        suggestion: result,
        type: suggestionType,
        hasSelection: !!selectedText
      };
    } catch (error) {
      console.error('Error generating suggestion:', error);
      throw new Error('Failed to generate AI suggestion');
    }
  }

  private async directAICall(prompt: string): Promise<string> {
    if (this.geminiApiKey) {
      return this.directGeminiCall(prompt);
    } else if (this.openaiApiKey) {
      return this.directOpenAICall(prompt);
    } else {
      throw new Error('No AI API key configured');
    }
  }

  private async directGeminiCall(prompt: string): Promise<string> {
    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${this.geminiApiKey}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        contents: [{
          parts: [{ text: prompt }]
        }],
        generationConfig: {
          temperature: 0.3,
          maxOutputTokens: 2000,
          topP: 0.8,
          topK: 40
        }
      }),
    });

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Gemini API error response:', errorText);
      throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    return data.candidates?.[0]?.content?.parts?.[0]?.text || 'No response generated';
  }

  private async directOpenAICall(prompt: string): Promise<string> {
    const openai = new OpenAI({ apiKey: this.openaiApiKey });

    const completion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo',
      messages: [{ role: 'user', content: prompt }],
      max_tokens: 2000,
      temperature: 0.3,
    });

    return completion.choices[0]?.message?.content || 'No response generated';
  }

  async applySuggestion(data: {
    noteId: string;
    originalContent: string;
    suggestion: string;
    selectedText?: string;
    applyType: 'replace' | 'append' | 'insert';
    position?: number;
  }, userId: string) {
    let newContent = data.originalContent;

    switch (data.applyType) {
      case 'replace':
        if (data.selectedText) {
          newContent = newContent.replace(data.selectedText, data.suggestion);
        } else {
          newContent = data.suggestion;
        }
        break;
      
      case 'append':
        newContent = newContent + '\n\n' + data.suggestion;
        break;
      
      case 'insert':
        const position = data.position || newContent.length;
        newContent = newContent.slice(0, position) + '\n\n' + data.suggestion + '\n\n' + newContent.slice(position);
        break;
    }

    return {
      newContent,
      applied: true,
      type: data.applyType
    };
  }
}